{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b243dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\"\"\"\n",
    "StepLR->step learning rate->every step what happen with multiplied with some constant term so that it will keep on reducing the the overall learning rates once its starts approaching to the minima. \n",
    "\"\"\"\n",
    "from torchvision import datasets, transforms\n",
    "import mlflow\n",
    "import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b8f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    EPOCHS=10\n",
    "    BATCH_SIZE=32\n",
    "    LR=0.01\n",
    "    DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    GAMMA=0.7\n",
    "    \"\"\"\n",
    "    GAMM value associated with StepLR-> after certain number of steps this GAMMA value will be multiplied with the learning rate LR. So that further reducing the LR\n",
    "    So lets say after 10 number of steps we can multiply 0.7 with LR=0.001. So this will give us further reduced value. Learning rate actually gets slowed down once we ieterate over the EPOCHS.\n",
    "    \n",
    "    \"\"\"\n",
    "    SEED=42\n",
    "    LOG_INTERVAL=10\n",
    "    \"\"\"\n",
    "    LOG_INTERval->how many intervals you want to log the output or print the outcome\n",
    "    \"\"\"\n",
    "    TEST_BATCH_SIZE=1000\n",
    "    \"\"\"\n",
    "    TEST_BATCH_SIZE->we can keep different batch size for the training and testing. So for testing we keep 1000 data point at once.\n",
    "    \"\"\"\n",
    "    DRY_RUN=True\n",
    "    \"\"\"\n",
    "    DRY_RUN->It can be True or False. Just to check wheter you are going thrugh entire training or the function that you implemented just once without iterating over the entire epochs\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12048fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76afa20",
   "metadata": {},
   "source": [
    "# Implement the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e97d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        import the super class all the possible constructor are there or the functions which are used or arguments which can be initialized here as a parent Networks.\n",
    "        \n",
    "        \"\"\"\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,32,3,1)#1is image. 32 is filter size, 3 is kernel size, 1 is stride\n",
    "        self.conv2=nn.Conv2d(32,64,3,1)\n",
    "        self.dropout1=nn.Dropout(0.25)\n",
    "        self.drpout2=nn.Dropout(0.50)\n",
    "        self.fc1=nn.Linear(9216,128)\n",
    "        self.fc2=nn.Linear(128,10)\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        connect with convolution layer one\n",
    "        \n",
    "        \"\"\"\n",
    "        x=self.conv1(x)\n",
    "        \"\"\"\n",
    "        after convolution we are going to use relu activation function. relu will take the x and it will pass to the next convolution layer\n",
    "        \n",
    "        \"\"\"\n",
    "        x=F.relu(x)\n",
    "        x=self.conv2(x)\n",
    "        x=F.relu(x)\n",
    "        x=f.max_pool2d(x,2)\n",
    "        x=self.dropout1(x)\n",
    "        x=torch.flatten(x,1)# 1 is dimension here\n",
    "        x=self.fc1(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.dropout2(x)\n",
    "        x=self.fc2(x)\n",
    "        \"\"\"\n",
    "        log_softmax will take the target value. thats mean whatever is the value is coming the raw value is  saying \n",
    "        apply a softmax and we are passing x and also dimension\n",
    "        \"\"\"\n",
    "        output=F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbea2e3",
   "metadata": {},
   "source": [
    "# Define the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ae4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, device,train_loader, optimizer, epochs):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bcb042",
   "metadata": {},
   "source": [
    "# Define the testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a115f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,device, test_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1693f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2255f52c090>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd766832",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs={\"batch_size\":config.BATCH_SIZE}\n",
    "test_kwargs={\"batch_size\":config.TEST_BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53633447",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.DEVICE==\"cuda\":\n",
    "    \"\"\"\n",
    "    cuda_kwargs->apply what are the number of workers that means how many cpu members which will be enagaged\n",
    "    \"\"\"\n",
    "    cuda_kwargs={\"num_workers\":1,\"pin_memory\":True,\"shupple\":True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50d0f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms=transforms.Compose(\n",
    "    [transforms.ToTensor()]#all the images converted to Tensor\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c421cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../demo\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52.7%"
     ]
    }
   ],
   "source": [
    "train=datasets.MNIST(\"../demo\", train=True, download=True, transform=transforms)\n",
    "test=datasets.MNIST(\"../demo\", train=False, download=True, transform=transforms)\n",
    "train_loader=torch.utils.data.DataLoader(train, **train_kwargs)\n",
    "test_loader=torch.utils.data.DataLoader(test, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f50e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
